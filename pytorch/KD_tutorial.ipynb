{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2020 NVIDIA Corporation\n",
      "Built on Mon_Oct_12_20:54:10_Pacific_Daylight_Time_2020\n",
      "Cuda compilation tools, release 11.1, V11.1.105\n",
      "Build cuda_11.1.relgpu_drvr455TC455_06.29190527_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization을 위해 mean을 빼고, standard deviation으로 나누려고 함\n",
    "\n",
    "transforms_cifar = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=False, transform=transforms_cifar)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=False, transform=transforms_cifar)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Defineing model classes and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LightNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)    \n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Classification task를 위한 2가지 함수 정의. \n",
    "model : 모델 인스턴스 생성\n",
    "\n",
    "train_loader : 모델에 데이터 주입시키기 위한 함수\n",
    "\n",
    "epochs : 데이터셋을 얼마나 반복할 것인지?\n",
    "\n",
    "learning_rate : 수렴되기 위해 스텝을 얼마나 크게할지 결정\n",
    "\n",
    "device : CPU 혹은 GPU로 돌리기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, learning_rate, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # input : 배치사이즈 이미지의 집합\n",
    "            # label : 각 이미지 클래스를 나타내는 정수값을 배치사이즈 차원만큼의 벡터로 정의\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval() # 평가모드로 전환\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # no_grad로 기울기 업데이트 안함\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            # torch.max로 가장 높은 값을 가진 인덱스 반환\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # 배치 내 샘플 개수를 반환. 즉 batch_size=32라면 32를 반환함\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, loss: 1.3376230263649045\n",
      "Epoch 2/10, loss: 0.8801519657339891\n",
      "Epoch 3/10, loss: 0.6984434832087563\n",
      "Epoch 4/10, loss: 0.5518185598466098\n",
      "Epoch 5/10, loss: 0.4360514838067467\n",
      "Epoch 6/10, loss: 0.32272086584049725\n",
      "Epoch 7/10, loss: 0.2277922832866764\n",
      "Epoch 8/10, loss: 0.1762107626994705\n",
      "Epoch 9/10, loss: 0.1397753664485329\n",
      "Epoch 10/10, loss: 0.12448156816537118\n",
      "Test Accuracy: 75.25%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42) # 실험 재현성을 위해 시드값 설정\n",
    "nn_deep = DeepNN(num_classes=10).to(device)\n",
    "train(nn_deep, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "test_accuracy_deep = test(nn_deep, test_loader, device)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "nn_light = LightNN(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 똑같은 모델을 하나 더 만들어서 같은 초기값인지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "new_nn_light = LightNN(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of 1st layer of nn_light: 2.327361822128296\n",
      "Norm of 1st layer of new_nn_light: 2.327361822128296\n"
     ]
    }
   ],
   "source": [
    "print(\"Norm of 1st layer of nn_light:\", torch.norm(nn_light.features[0].weight).item())\n",
    "print(\"Norm of 1st layer of new_nn_light:\", torch.norm(new_nn_light.features[0].weight).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepNN parameters: 1,186,986\n",
      "LightNN parameters: 267,738\n"
     ]
    }
   ],
   "source": [
    "total_params_deep = \"{:,}\".format(sum(p.numel() for p in nn_deep.parameters()))\n",
    "print(f\"DeepNN parameters: {total_params_deep}\")\n",
    "total_params_light = \"{:,}\".format(sum(p.numel() for p in nn_light.parameters()))\n",
    "print(f\"LightNN parameters: {total_params_light}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, loss: 1.4702755151807194\n",
      "Epoch 2/10, loss: 1.1610109941733768\n",
      "Epoch 3/10, loss: 1.025268422825562\n",
      "Epoch 4/10, loss: 0.9218627793709641\n",
      "Epoch 5/10, loss: 0.8466211883613216\n",
      "Epoch 6/10, loss: 0.7808126970325284\n",
      "Epoch 7/10, loss: 0.7174562125864541\n",
      "Epoch 8/10, loss: 0.6567044667423229\n",
      "Epoch 9/10, loss: 0.6046204406129735\n",
      "Epoch 10/10, loss: 0.5547780798524237\n",
      "Test Accuracy: 70.76%\n"
     ]
    }
   ],
   "source": [
    "# light_nn을 cross entropy loss을 통해 훈련 및 테스트\n",
    "train(nn_light, train_loader, epochs=10, learning_rate=0.001, device=device)\n",
    "test_accuracy_light_nn = test(nn_light, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같은 결과를 토대로 더 깊은 모델은 teacher가 될 수 있고, 가벼운 모델은 student 모델이 될 수 있음. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher accuracy: 75.25%\n",
      "Student accuracy: 70.76%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy: {test_accuracy_light_nn:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Knowledge Distillation RUN\n",
    "> ### Teacher를 통해 Student의 accuracy를 개선\n",
    "지식 증류는 추가적인 loss를 기존 cross entropy(teacher 모델의 softmax 출력 기반)과 함께 작동됨.\n",
    "\n",
    "지식은 정답인 label 외에도 확률이 존재하는 레이블에 대해 전체적으로 학습할 수 있도록 soft label을 활용함. \n",
    "\n",
    "하지만, cross entropy는 이러한 정보를 효율적으로 이용하지 않음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, \n",
    "                                 T, soft_target_loss_weight, ce_loss_weight, device):\n",
    "    ce_loss = nn.CrossEntropyLoss() # task와의 loss function\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate) # optimizer Adam 활용\n",
    "    \n",
    "    teacher.eval() # teacher는 더이상 학습하지 않기 때문에 eval 모드\n",
    "    student.train() # student는 task 및 teacher와의 학습을 위해 train 모드\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader: \n",
    "            inputs, labels = inputs.to(device), labels.to(device) # GPU 메모리 내에서 실행되도록 to(device) 수행\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "            \n",
    "            student_logits = student(inputs)\n",
    "            \n",
    "            # teacher logit을 soft하게 바꿈\n",
    "            soft_targets = nn.functional.softmax(teacher_logits/T, dim=-1)\n",
    "            # student logit은 log softmax를 이용하여 손실 함수 계산의 원활함 제공\n",
    "            soft_prob = nn.functional.log_softmax(student_logits/T, dim=-1)\n",
    "            \n",
    "            # Distilling the knowledge in a neural network 논문에 따라 T^2로 soft target loss를 스케일링\n",
    "            soft_targets_loss = -torch.sum(soft_targets * soft_prob) / soft_prob.size()[0] * (T**2)\n",
    "            \n",
    "            # KL Divergence 방법\n",
    "            soft_targets_loss_kl = nn.functional.kl_div(soft_prob, soft_targets, reduction='batchmean') * (T**2)\n",
    "            \n",
    "            # ground truth 손실 계산\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "            \n",
    "            # 두가지 loss의 Weight Sum(weighted 계수 : soft_target_loss_weight, ce_loss_weight)\n",
    "            # loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
    "            loss = soft_target_loss_weight * soft_targets_loss_kl + ce_loss_weight * label_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### KD 학습을 위한 하이퍼파라미터\n",
    "> ### T : 2, Distillation Loss(Soft_target_loss_weight), Ground Truth와 Student 간의 Cross Entropy를 위한 0.75로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.3522917474322305\n",
      "Epoch 2/10, Loss: 1.8351799155135289\n",
      "Epoch 3/10, Loss: 1.6122780648033943\n",
      "Epoch 4/10, Loss: 1.452798021723852\n",
      "Epoch 5/10, Loss: 1.3281061784995487\n",
      "Epoch 6/10, Loss: 1.2145687071868525\n",
      "Epoch 7/10, Loss: 1.1228389831455163\n",
      "Epoch 8/10, Loss: 1.0379482465022056\n",
      "Epoch 9/10, Loss: 0.9645864333947907\n",
      "Epoch 10/10, Loss: 0.8997774232379006\n",
      "Test Accuracy: 70.77%\n"
     ]
    }
   ],
   "source": [
    "train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, train_loader=train_loader,\n",
    "                             epochs=10, learning_rate=0.001, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device) # soft target weight 및 ce weight는 하이퍼파라미터 탐색으로 찾기\n",
    "test_accuracy_light_ce_kd = test(new_nn_light, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher accuracy: 75.25%\n",
      "Student accuracy without teacher : 70.76%\n",
      "Student accuracy with CE + KD: 70.77%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy without teacher : {test_accuracy_light_nn:.2f}%\")\n",
    "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_kd:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Teacher의 내부 표현을 흉내내기 위한 지식 증류는 문제점이 있다면, 학습 수용량이 student와 teacher가 다름.\n",
    "> ### 내적 표현 흉내내기를 위해 CosineEmbedding Loss를 적용해서 영향력을 알아보려고함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### output layer를 증류시키기 위해서는 같은 수의 뉴런이 있어야함.\n",
    "> ### 하지만, Teacher 모델의 뉴런수는 student에 비해 많이 때문에(flatten 했을 때) 매칭을 시켜주는 작업이 필요함. -> loss function이 같은 차원의 벡터가 입력으로 들어가야하는 조건\n",
    "> ### Student의 차원에 맞춰주기 위해, Average Pooling을 적용하여 Teacher의 차원을 줄일 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedDeepNNCosine(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModifiedDeepNNCosine, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)    \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        flattened_conv_output = torch.flatten(x, 1).unsqueeze(1)\n",
    "        x = self.classifier(flattened_conv_output)\n",
    "        flattened_conv_output_after_pooling = torch.nn.functional.avg_pool1d(flattened_conv_output, 2)\n",
    "        \n",
    "        return x, flattened_conv_output_after_pooling\n",
    "    \n",
    "\n",
    "class ModifiedLightNNCosine(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModifiedLightNNCosine, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        flatten = torch.flatten(x, 1)\n",
    "        x = self.classifier(flatten)\n",
    "        \n",
    "        return x, flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm of 1st layer deep_nn 7.515564441680908\n",
      "Norm of 1st layer modified_deep_nn 7.515564441680908\n"
     ]
    }
   ],
   "source": [
    "modified_deepNN = ModifiedDeepNNCosine(num_classes=10).to(device)\n",
    "modified_deepNN.load_state_dict(nn_deep.state_dict())\n",
    "\n",
    "\n",
    "# 첫번째 layer의 가중치가 같은지 확인\n",
    "\n",
    "print(\"Norm of 1st layer deep_nn\", torch.norm(nn_deep.features[0].weight).item())\n",
    "print(\"Norm of 1st layer modified_deep_nn\", torch.norm(modified_deepNN.features[0].weight).item())\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "modified_lightNN = ModifiedLightNNCosine(num_classes=10).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 기존과 달리 hidden 표현을 return 해줘야하기 때문에 train loop이 바뀌어야함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightNN\n",
      "torch.Size([128, 10])\n",
      "torch.Size([128, 1024])\n",
      "deepNN\n",
      "torch.Size([128, 10])\n",
      "torch.Size([128, 1024])\n"
     ]
    }
   ],
   "source": [
    "sample_input = torch.randn(128, 3, 32, 32).to(device)\n",
    "\n",
    "logits, hidden_representation = modified_lightNN(sample_input)\n",
    "\n",
    "print(\"lightNN\")\n",
    "print(logits.shape)\n",
    "print(hidden_representation.shape)\n",
    "\n",
    "logits, hidden_representation = modified_deepNN(sample_input)\n",
    "logits = logits.reshape(128, 10)\n",
    "hidden_representation = hidden_representation.reshape(128, 1024)\n",
    "\n",
    "\n",
    "print(\"deepNN\")\n",
    "print(logits.shape)\n",
    "print(hidden_representation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine loss minimization에서 Teacher와 Student 간의 cosine similarity를 극대화 해야함\n",
    "\n",
    "def train_cosine_loss(teacher, student, train_loader, epochs, learning_rate, hidden_rep_loss_weight, ce_loss_weight, device):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    cosine_loss = nn.CosineEmbeddingLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # GPU 메모리로 model 전달 및 teacher은 검증용, student는 학습용으로 진행\n",
    "    teacher.to(device)\n",
    "    student.to(device)\n",
    "    teacher.eval()\n",
    "    student.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                _, teacher_hidden_representation = teacher(inputs)\n",
    "                \n",
    "            student_logits, student_hidden_representation = student(inputs)\n",
    "            teacher_hidden_representation = teacher_hidden_representation.reshape(-1, 1024)\n",
    "            \n",
    "            # 중간층에서의 loss를 구하는 식은 feature based knowledge이 맞지않나? \n",
    "            hidden_rep_loss = cosine_loss(student_hidden_representation, teacher_hidden_representation, target=torch.ones(inputs.size(0)).to(device))\n",
    "            \n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "            \n",
    "            loss = hidden_rep_loss_weight * hidden_rep_loss + ce_loss_weight * label_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss:{running_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_multiple_outputs(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs, _ = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss:1.306582652394424\n",
      "Epoch 2/10, Loss:1.0682969283874688\n",
      "Epoch 3/10, Loss:0.9670999458683726\n",
      "Epoch 4/10, Loss:0.8919807530729972\n",
      "Epoch 5/10, Loss:0.8381563493662783\n",
      "Epoch 6/10, Loss:0.7938108398481403\n",
      "Epoch 7/10, Loss:0.7531564029891168\n",
      "Epoch 8/10, Loss:0.7165703799413599\n",
      "Epoch 9/10, Loss:0.6806020036987637\n",
      "Epoch 10/10, Loss:0.6526263821155519\n",
      "Test Accuracy: 70.40%\n"
     ]
    }
   ],
   "source": [
    "train_cosine_loss(teacher=modified_deepNN, student=modified_lightNN, train_loader=train_loader, epochs=10, learning_rate=0.001, hidden_rep_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
    "test_accuracy_lightNN = test_multiple_outputs(modified_lightNN, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Intermediate regressor run\n",
    "> ### 우리가 다룰려는 벡터의 크기는 1024로 유사성 정보 추출이 힘듦. 또한, 1 대 1 매칭을 목표로 하는 것은 좋은 이유가 될 수 없음\n",
    "> ### Regressor로 문제점을 개선해보려고 함.\n",
    "> ### 1차적으로 Teacher 모델의 Conv 층의 Feature map을 추출하고, Student도 똑같이 feature map 추출 수행\n",
    "> ### 2차적으로 Teacher 및 Student의 feature map을 맞추는 과정이 필요함. 이를 위해 Regressor를 활용(학습 가능)\n",
    "> ### Regressor는 Teacher와 Student와의 차원성을 맵핑해주는 역할을 함.\n",
    "> ### 이를 활용한 Loss Function은 Teaching \"Path\"를 제공해주며 Student의 가중치 업데이트에 기여함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student feature : torch.Size([128, 16, 8, 8])\n",
      "teacher feature : torch.Size([128, 32, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# convolution feature 추출기로부터 얻어지는 결과물 차이\n",
    "\n",
    "conv_feature_student = nn_light.features(sample_input)\n",
    "conv_feature_teacher = nn_deep.features(sample_input)\n",
    "\n",
    "print(\"student feature :\", conv_feature_student.shape)\n",
    "print(\"teacher feature :\", conv_feature_teacher.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedDeepNNRegressor(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModifiedDeepNNRegressor, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        conv_feature_map = x\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x, conv_feature_map\n",
    "    \n",
    "\n",
    "class ModifiedLightNNRegressor(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModifiedLightNNRegressor, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        regressor_output = self.regressor(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x, regressor_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Regressor를 포함함에 따라 train loop의 변동이 필요함.\n",
    "> ### Student의 regressor output을 추출 및 teacher의 feature map을 추출하기 위해 MSE를 활용(같은 크기여야함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
