{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import norm\n",
    "import scipy\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    '''\n",
    "    computes and stores the average and current value\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1, )):\n",
    "    '''\n",
    "    Computes the precision for the specified values of k\n",
    "    '''\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(dataset, bs):\n",
    "    print(\"==> Preparing data..\")\n",
    "    \n",
    "    if dataset == 'cifar100':\n",
    "        mean = [x/255 for x in [129.3, 124.1, 112.4]]\n",
    "        std = [x/255 for x in [68.2, 65.4, 70.4]]\n",
    "    else:\n",
    "        assert False, f\"Unknown dataset : {dataset}\"\n",
    "        \n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    \n",
    "    if dataset == 'cifar100':\n",
    "        trainset = torchvision.datasets.CIFAR100(root='../data', train=True, download=True, transform=transform_train)\n",
    "        \n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=4)\n",
    "    \n",
    "    if dataset == 'cifar100':\n",
    "        testset = torchvision.datasets.CIFAR100(root='../data', train=False, download=True, transform=transform_test)\n",
    "        \n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=4)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, device, data_loader, criterion):\n",
    "    \n",
    "    top1 = AverageMeter()\n",
    "    net.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in enumerate(data_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            outputs = outputs.float()\n",
    "            prec1 = accuracy(outputs.data, targets)[0]\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion=1\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None, is_last=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = conv3(in_planes, planes, stride) # 들어가는 차원의 크기 in_planes 에서 나가는 planes\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=False) # inplace=False 원본 데이터 보존하는 대신 추가메모리 사용,\n",
    "        # 만약, inplace=True라면 들어가는 인수 또한 값이 output과 동일하게 바뀌는 현상 발생\n",
    "        self.conv2 = conv3(planes, planes) # 들낙하는 차원은 planes로 fix\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # downsample이 왜 필요한가? : 입력 텐서와 출력 텐서의 차원 불일치 문제 해소\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        # out은 잔차와 더해져서 다음 out은 relu를 거친것이고, preact는 relu 거치기전(pre-relu)\n",
    "        preact = out.clone() # out.clone()은 무슨 의미인가? copy()와 같은 의미인가? Pytorch에서 텐서 복사할때 clone 메서드 사용\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        if self.is_last:\n",
    "            # is_last가 각 블록마다의 distillation해야할 feature를 내보내야할 시기인가?\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, depth, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        # assert 가 경고한다는건데, ResNet-20, 32, 44 이런식으로 맞추는게 필요한가?\n",
    "        assert (depth - 2) % 6 == 0, 'depth should be one of 20, 32, 44, 56, 110'\n",
    "        block_num = (depth - 2) // 6 # block_num??\n",
    "        self.in_planes = 16\n",
    "        self.conv1 = conv3(in_planes=3, out_planes=16)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.layer1 = self._make_layer(planes=16, block_num=block_num)\n",
    "        self.layer2 = self._make_layer(planes=32, block_num=block_num, stride=2)\n",
    "        self.layer3 = self._make_layer(planes=64, block_num=block_num, stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "        \n",
    "        for m in self.modules(): # ResNet 모델의 모든 Conv층, BN층을 접근해서 가중치 초기화 수행\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # Conv층 같은 경우, fan_out(출력 유닛에 초점) <-> fan_in(입력 유닛에 초점) 을 사용\n",
    "                # 그리고 ReLU 활성화함수를 거침에 따라 nonlinearity를 'relu'로 설정\n",
    "                # kaiming_normal_ == He초기화\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "    \n",
    "    def _make_layer(self, planes, block_num, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes:\n",
    "            # stride가 1이 아니거나 in_planes와 planes가 같지 않다면 downsample 변동\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(BasicBlock(self.in_planes, planes, stride, downsample))\n",
    "        self.in_planes = planes\n",
    "        for i in range(1, block_num):\n",
    "            layers.append(BasicBlock(self.in_planes, planes))\n",
    "        return nn.Sequential(*layers) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = F.relu(x) ## nn.ReLU() 은 모듈 객체를 생성함에 따라 __init__에서 주로 쓰이고, F.relu는 모듈 객체를 생성하지 않으므로 forward에서 자주 쓰임\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1) # == x.flatten(start_dim=1), 1차원으로 평탄화, view 쓰는 경우에는 복잡한 차원 재구성이 필요할때\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def get_bn_before_relu(self):\n",
    "        \n",
    "        if isinstance(self.layer1[0], BasicBlock):\n",
    "            bn1 = self.layer1[-1].bn2\n",
    "            bn2 = self.layer2[-1].bn2\n",
    "            bn3 = self.layer3[-1].bn2\n",
    "        else:\n",
    "            print('ResNet unknown block error')\n",
    "        \n",
    "        return [bn1, bn2, bn3]\n",
    "    \n",
    "    def get_channel_num(self):\n",
    "        \n",
    "        return [16, 32, 64]\n",
    "    \n",
    "    def extract_feature(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        feat1 = self.layer1(x)\n",
    "        feat2 = self.layer2(feat1)\n",
    "        feat3 = self.layer3(feat2)\n",
    "        \n",
    "        x = nn.ReLU(inplace=False)(feat3)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return [feat1, feat2, feat3], out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet20(class_num=10):\n",
    "    return ResNet(20, class_num)\n",
    "    \n",
    "def resnet32(class_num=10):\n",
    "    return ResNet(32, class_num)\n",
    "    \n",
    "def resnet44(class_num=10):\n",
    "    return ResNet(44, class_num)\n",
    "\n",
    "def resnet56(class_num=10):\n",
    "    return ResNet(56, class_num)\n",
    "\n",
    "# 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = build_dataset('cifar100', 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distillation_loss(source, target, margin):\n",
    "    loss = ((source - margin)**2 * ((source > margin) & (target <= margin)).float()\n",
    "            + (source - target) ** 2 * ((source > target) & (target > margin) & (target <= 0)).float() + \n",
    "            (source - target) ** 2 * (target > 0).float())\n",
    "    # loss function을 어떻게 이해하면 될까...\n",
    "    return torch.abs(loss).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_connector(t_channel, s_channel):\n",
    "    # Teacher와 Student 간의 Feature Distillation을 위한 connector 함수\n",
    "    C = [nn.Conv2d(s_channel, t_channel, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "         nn.BatchNorm2d(t_channel)\n",
    "         ]\n",
    "\n",
    "    for m in C:\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            # He가중치를 쓰는 상황이므로 아래와 같이도 쓸 수 있음\n",
    "            # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "    return nn.Sequential(*C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_margin_from_BN(bn):\n",
    "    margin = []\n",
    "    std = bn.weight.data\n",
    "    mean = bn.bias.data\n",
    "    for (s, m) in zip(std, mean):\n",
    "        s = abs(s.item())\n",
    "        m = m.item()\n",
    "        if norm.cdf(-m / s) > 0.001:\n",
    "            margin.append(-s * math.exp(-(m/s) ** 2 / 2) / math.sqrt(2 * math.pi) / norm.cdf(-m / s) + m)\n",
    "        else:\n",
    "            margin.append(-3 * s)\n",
    "    \n",
    "    return torch.FloatTensor(margin).to(std.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distiller(nn.Module):\n",
    "    def __init__(self, t_net, s_net):\n",
    "        super(Distiller, self).__init__()\n",
    "        \n",
    "        t_channels = t_net.get_channel_num()\n",
    "        s_channels = s_net.get_channel_num()\n",
    "        \n",
    "        # build_feature_connector에서 리스트로 묶은 레이어가 있어서 ModuleList로 정의해줘야함.\n",
    "        # zip을 쓰는 이유는 동시순회를 위해서임\n",
    "        self.Connectors = nn.ModuleList([build_feature_connector(t, s) for t, s in zip(t_channels, s_channels)])\n",
    "        \n",
    "        teacher_bns = t_net.get_bn_before_relu()\n",
    "        margins = [get_margin_from_BN(bn) for bn in teacher_bns]\n",
    "        for i, margin in enumerate(margins):\n",
    "            self.register_buffer('margin%d' % (i + 1), margin.unsqueeze(1).unsqueeze(2).unsqueeze(0).detach())\n",
    "            \n",
    "        self.t_net = t_net\n",
    "        self.s_net = s_net\n",
    "    \n",
    "    def forward(self, x):\n",
    "        t_feats, t_out = self.t_net.extract_feature(x)\n",
    "        s_feats, s_out = self.s_net.extract_feature(x)\n",
    "        feat_num = len(t_feats)\n",
    "        \n",
    "        loss_distill = 0\n",
    "        for i in range(feat_num):\n",
    "            s_feats[i] = self.Connectors[i](s_feats[i])\n",
    "            loss_distill += distillation_loss(s_feats[i], t_feats[i].detach(), getattr(self, 'margin%d' %(i+1))) / 2 ** (feat_num - i - 1)\n",
    "        \n",
    "        return s_out, loss_distill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies=[]\n",
    "test_accuracies=[]\n",
    "class_num = 100\n",
    "\n",
    "# Teacher / Student Network 정의 \n",
    "t_net = resnet56(100).to(device)\n",
    "s_net = resnet20(100).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Teacher / Student Network 끼리의 Distillation을 위한 Distiller 정의\n",
    "d_net = Distiller(t_net, s_net)\n",
    "\n",
    "optimizer = optim.Adam([{'params' : s_net.parameters()}, {'params' : d_net.Connectors.parameters()}], lr=0.05) # Momentum, weight_decay는 생략"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_distill(d_net, optimizer, device, train_loader, criterion):\n",
    "    d_net.to(device)\n",
    "    d_net.s_net.to(device)\n",
    "    d_net.t_net.to(device)\n",
    "    d_net.train()\n",
    "    d_net.s_net.train()\n",
    "    d_net.t_net.train()\n",
    "    top1 = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        \n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        batch_size = inputs.shape[0]\n",
    "        outputs, loss_distill = d_net(inputs)\n",
    "        loss_CE = criterion(outputs, targets)\n",
    "        loss = loss_CE + 1e-4 * loss_distill.sum() / batch_size\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        outputs = outputs.float()\n",
    "        prec1 = accuracy(outputs.data, targets)[0]\n",
    "        top1.update(prec1.item(), inputs.size(0))\n",
    "    return top1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_teacher(t_net, optimizer, device, train_loader, criterion):\n",
    "    t_net.to(device)\n",
    "    t_net.train()\n",
    "    \n",
    "    print(\"training...\")\n",
    "    for epoch in range(150):\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            outputs = t_net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{150}, loss: {running_loss / len(train_loader)}\")\n",
    "            \n",
    "    return t_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "Epoch 1/150, loss: 5.350280089756412\n",
      "Epoch 2/150, loss: 5.350695907612286\n",
      "Epoch 3/150, loss: 5.351594646873377\n",
      "Epoch 4/150, loss: 5.350470336806744\n",
      "Epoch 5/150, loss: 5.349426378069631\n",
      "Epoch 6/150, loss: 5.351610732505389\n",
      "Epoch 7/150, loss: 5.349905806734129\n",
      "Epoch 8/150, loss: 5.352293780392698\n",
      "Epoch 9/150, loss: 5.351639112243262\n",
      "Epoch 10/150, loss: 5.349537050632565\n",
      "Epoch 11/150, loss: 5.35118992920117\n",
      "Epoch 12/150, loss: 5.350918945449088\n",
      "Epoch 13/150, loss: 5.348647307861796\n",
      "Epoch 14/150, loss: 5.350719018970304\n",
      "Epoch 15/150, loss: 5.349038360673753\n",
      "Epoch 16/150, loss: 5.351611809352475\n",
      "Epoch 17/150, loss: 5.349895082166433\n",
      "Epoch 18/150, loss: 5.350416955435672\n",
      "Epoch 19/150, loss: 5.351166100758116\n",
      "Epoch 20/150, loss: 5.350278601926916\n",
      "Epoch 21/150, loss: 5.351544472872448\n",
      "Epoch 22/150, loss: 5.351359635667728\n",
      "Epoch 23/150, loss: 5.35009429643831\n",
      "Epoch 24/150, loss: 5.3509192893572175\n",
      "Epoch 25/150, loss: 5.3514827255092925\n",
      "Epoch 26/150, loss: 5.351053752557701\n",
      "Epoch 27/150, loss: 5.351517954140978\n",
      "Epoch 28/150, loss: 5.350587231423849\n",
      "Epoch 29/150, loss: 5.3496245398850695\n",
      "Epoch 30/150, loss: 5.350668875457686\n",
      "Epoch 31/150, loss: 5.349975023733076\n",
      "Epoch 32/150, loss: 5.350706106561529\n",
      "Epoch 33/150, loss: 5.350406346723552\n",
      "Epoch 34/150, loss: 5.351899338500274\n",
      "Epoch 35/150, loss: 5.351073289466331\n",
      "Epoch 36/150, loss: 5.350637118834669\n",
      "Epoch 37/150, loss: 5.350531831726698\n",
      "Epoch 38/150, loss: 5.350231292607535\n",
      "Epoch 39/150, loss: 5.3510020017014135\n",
      "Epoch 40/150, loss: 5.351612589853194\n",
      "Epoch 41/150, loss: 5.350531653674972\n",
      "Epoch 42/150, loss: 5.350448866939301\n",
      "Epoch 43/150, loss: 5.350748546288141\n",
      "Epoch 44/150, loss: 5.351544415554427\n",
      "Epoch 45/150, loss: 5.351864980614704\n",
      "Epoch 46/150, loss: 5.348506684803292\n",
      "Epoch 47/150, loss: 5.3497007728537636\n",
      "Epoch 48/150, loss: 5.351837575283197\n",
      "Epoch 49/150, loss: 5.349135720821293\n",
      "Epoch 50/150, loss: 5.349614381180395\n",
      "Epoch 51/150, loss: 5.351021699588317\n",
      "Epoch 52/150, loss: 5.349853129033238\n",
      "Epoch 53/150, loss: 5.35022717546624\n",
      "Epoch 54/150, loss: 5.351306336005326\n",
      "Epoch 55/150, loss: 5.350325506361549\n",
      "Epoch 56/150, loss: 5.351082729866437\n",
      "Epoch 57/150, loss: 5.348281565224728\n",
      "Epoch 58/150, loss: 5.350363192350968\n",
      "Epoch 59/150, loss: 5.35004191752285\n",
      "Epoch 60/150, loss: 5.351403857131139\n",
      "Epoch 61/150, loss: 5.350752990264112\n",
      "Epoch 62/150, loss: 5.350161271936753\n",
      "Epoch 63/150, loss: 5.351557041373094\n",
      "Epoch 64/150, loss: 5.349412253445677\n",
      "Epoch 65/150, loss: 5.350139110594454\n",
      "Epoch 66/150, loss: 5.349743267459333\n",
      "Epoch 67/150, loss: 5.349933907199089\n",
      "Epoch 68/150, loss: 5.349615315342193\n",
      "Epoch 69/150, loss: 5.350068520402055\n",
      "Epoch 70/150, loss: 5.350822620684533\n",
      "Epoch 71/150, loss: 5.350073142429752\n",
      "Epoch 72/150, loss: 5.349232601692609\n",
      "Epoch 73/150, loss: 5.3501416496608565\n",
      "Epoch 74/150, loss: 5.3504440290543736\n",
      "Epoch 75/150, loss: 5.350555107721587\n",
      "Epoch 76/150, loss: 5.350409611411717\n",
      "Epoch 77/150, loss: 5.351322922865143\n",
      "Epoch 78/150, loss: 5.350594459592229\n",
      "Epoch 79/150, loss: 5.3499407426780445\n",
      "Epoch 80/150, loss: 5.350955282635701\n",
      "Epoch 81/150, loss: 5.34909568479299\n",
      "Epoch 82/150, loss: 5.349851092414173\n",
      "Epoch 83/150, loss: 5.350096586720108\n",
      "Epoch 84/150, loss: 5.351318442303201\n",
      "Epoch 85/150, loss: 5.3500429553449\n",
      "Epoch 86/150, loss: 5.347871699906372\n",
      "Epoch 87/150, loss: 5.350163131723623\n",
      "Epoch 88/150, loss: 5.350494595744726\n",
      "Epoch 89/150, loss: 5.35120977952962\n",
      "Epoch 90/150, loss: 5.3488722335347125\n",
      "Epoch 91/150, loss: 5.350300409604826\n",
      "Epoch 92/150, loss: 5.350813881515542\n",
      "Epoch 93/150, loss: 5.351584268652874\n",
      "Epoch 94/150, loss: 5.350766430730405\n",
      "Epoch 95/150, loss: 5.352266455550328\n",
      "Epoch 96/150, loss: 5.351212108836455\n",
      "Epoch 97/150, loss: 5.348952725110457\n",
      "Epoch 98/150, loss: 5.350380147509562\n",
      "Epoch 99/150, loss: 5.3499189776837675\n",
      "Epoch 100/150, loss: 5.347444240394456\n",
      "Epoch 101/150, loss: 5.3510400657458685\n",
      "Epoch 102/150, loss: 5.351361025934634\n",
      "Epoch 103/150, loss: 5.350650839793408\n",
      "Epoch 104/150, loss: 5.350652211767328\n",
      "Epoch 105/150, loss: 5.350357760553774\n",
      "Epoch 106/150, loss: 5.350258001586056\n",
      "Epoch 107/150, loss: 5.349373906469711\n",
      "Epoch 108/150, loss: 5.348557241737385\n",
      "Epoch 109/150, loss: 5.3492349590486885\n",
      "Epoch 110/150, loss: 5.349254224001599\n",
      "Epoch 111/150, loss: 5.3493365473149685\n",
      "Epoch 112/150, loss: 5.3494630994089425\n",
      "Epoch 113/150, loss: 5.352027234518924\n",
      "Epoch 114/150, loss: 5.350289250883605\n",
      "Epoch 115/150, loss: 5.351288501563889\n",
      "Epoch 116/150, loss: 5.350140904526576\n",
      "Epoch 117/150, loss: 5.350409629704703\n",
      "Epoch 118/150, loss: 5.351533162929213\n",
      "Epoch 119/150, loss: 5.351850807209454\n",
      "Epoch 120/150, loss: 5.350429942235922\n",
      "Epoch 121/150, loss: 5.3503375724148565\n",
      "Epoch 122/150, loss: 5.351848164482799\n",
      "Epoch 123/150, loss: 5.349312485941231\n",
      "Epoch 124/150, loss: 5.3515506790727\n",
      "Epoch 125/150, loss: 5.350020678147025\n",
      "Epoch 126/150, loss: 5.350411250463227\n",
      "Epoch 127/150, loss: 5.350509293548896\n",
      "Epoch 128/150, loss: 5.349497658517355\n",
      "Epoch 129/150, loss: 5.352132689922362\n",
      "Epoch 130/150, loss: 5.351403418099484\n",
      "Epoch 131/150, loss: 5.349677426125997\n",
      "Epoch 132/150, loss: 5.35006356056389\n",
      "Epoch 133/150, loss: 5.350563461518349\n",
      "Epoch 134/150, loss: 5.350911174588801\n",
      "Epoch 135/150, loss: 5.348560772283608\n",
      "Epoch 136/150, loss: 5.350127411620392\n",
      "Epoch 137/150, loss: 5.349093570123853\n",
      "Epoch 138/150, loss: 5.350856179776399\n",
      "Epoch 139/150, loss: 5.351696432703901\n",
      "Epoch 140/150, loss: 5.351264591412166\n",
      "Epoch 141/150, loss: 5.349620795920682\n",
      "Epoch 142/150, loss: 5.34926284487595\n",
      "Epoch 143/150, loss: 5.350577716632268\n",
      "Epoch 144/150, loss: 5.352081535417406\n",
      "Epoch 145/150, loss: 5.350271995720046\n",
      "Epoch 146/150, loss: 5.349760689698827\n",
      "Epoch 147/150, loss: 5.350828384194533\n",
      "Epoch 148/150, loss: 5.351019205644612\n",
      "Epoch 149/150, loss: 5.3502395610370295\n",
      "Epoch 150/150, loss: 5.350421586000096\n"
     ]
    }
   ],
   "source": [
    "trained_t_net = train_teacher(t_net, optimizer, device, train_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trained_t_net.state_dict(), \"../models/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch=0\n",
    "start = time.time()\n",
    "for epoch in range(start_epoch, 150):\n",
    "    if epoch in [80, 120]:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.1 \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
