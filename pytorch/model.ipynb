{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion=1\n",
    "    def __init__(self, in_planes, planes, stride=1, downsample=None, is_last=False):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.is_last = is_last\n",
    "        self.conv1 = conv3(in_planes, planes, stride) # 들어가는 차원의 크기 in_planes 에서 나가는 planes\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=False) # inplace=False 원본 데이터 보존하는 대신 추가메모리 사용,\n",
    "        # 만약, inplace=True라면 들어가는 인수 또한 값이 output과 동일하게 바뀌는 현상 발생\n",
    "        self.conv2 = conv3(planes, planes) # 들낙하는 차원은 planes로 fix\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # downsample이 왜 필요한가? : 입력 텐서와 출력 텐서의 차원 불일치 문제 해소\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        # out은 잔차와 더해져서 다음 out은 relu를 거친것이고, preact는 relu 거치기전(pre-relu)\n",
    "        preact = out.clone() # out.clone()은 무슨 의미인가? copy()와 같은 의미인가? Pytorch에서 텐서 복사할때 clone 메서드 사용\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        if self.is_last:\n",
    "            # is_last가 각 블록마다의 distillation해야할 feature를 내보내야할 시기인가?\n",
    "            return out, preact\n",
    "        else:\n",
    "            return out\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, depth, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        # assert 가 경고한다는건데, ResNet-20, 32, 44 이런식으로 맞추는게 필요한가?\n",
    "        assert (depth - 2) % 6 == 0, 'depth should be one of 20, 32, 44, 56, 110'\n",
    "        block_num = (depth - 2) // 6 # block_num??\n",
    "        self.in_planes = 16\n",
    "        self.conv1 = conv3(in_planes=3, out_planes=16)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.layer1 = self._make_layer(planes=16, block_num=block_num)\n",
    "        self.layer2 = self._make_layer(planes=32, block_num=block_num, stride=2)\n",
    "        self.layer3 = self._make_layer(planes=64, block_num=block_num, stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "        \n",
    "        for m in self.modules(): # ResNet 모델의 모든 Conv층, BN층을 접근해서 가중치 초기화 수행\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # Conv층 같은 경우, fan_out(출력 유닛에 초점) <-> fan_in(입력 유닛에 초점) 을 사용\n",
    "                # 그리고 ReLU 활성화함수를 거침에 따라 nonlinearity를 'relu'로 설정\n",
    "                # kaiming_normal_ == He초기화\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "    \n",
    "    def _make_layer(self, planes, block_num, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_planes != planes:\n",
    "            # stride가 1이 아니거나 in_planes와 planes가 같지 않다면 downsample 변동\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_planes, planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "        layers = []\n",
    "        layers.append(BasicBlock(self.in_planes, planes, stride, downsample))\n",
    "        self.in_planes = planes\n",
    "        for i in range(1, block_num):\n",
    "            layers.append(BasicBlock(self.in_planes, planes))\n",
    "        return nn.Sequential(*layers) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = F.relu(x) ## nn.ReLU() 은 모듈 객체를 생성함에 따라 __init__에서 주로 쓰이고, F.relu는 모듈 객체를 생성하지 않으므로 forward에서 자주 쓰임\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1) # == x.flatten(start_dim=1), 1차원으로 평탄화, view 쓰는 경우에는 복잡한 차원 재구성이 필요할때\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def get_bn_before_relu(self):\n",
    "        \n",
    "        if isinstance(self.layer1[0], BasicBlock):\n",
    "            bn1 = self.layer1[-1].bn2\n",
    "            bn2 = self.layer2[-1].bn2\n",
    "            bn3 = self.layer3[-1].bn2\n",
    "        else:\n",
    "            print('ResNet unknown block error')\n",
    "        \n",
    "        return [bn1, bn2, bn3]\n",
    "    \n",
    "    def get_channel_num(self):\n",
    "        \n",
    "        return [16, 32, 64]\n",
    "    \n",
    "    def extract_feature(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        feat1 = self.layer1(x)\n",
    "        feat2 = self.layer2(feat1)\n",
    "        feat3 = self.layer3(feat2)\n",
    "        \n",
    "        x = nn.ReLU(inplace=False)(feat3)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return [feat1, feat2, feat3], out\n",
    "    \n",
    "def resnet20(class_num=10):\n",
    "    return ResNet(20, class_num)\n",
    "    \n",
    "def resnet32(class_num=10):\n",
    "    return ResNet(32, class_num)\n",
    "    \n",
    "def resnet44(class_num=10):\n",
    "    return ResNet(44, class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.resnet20(class_num=10)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_net = resnet20\n",
    "t_net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
